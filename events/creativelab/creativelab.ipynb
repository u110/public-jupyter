{"cells":[{"metadata":{"_uuid":"ac12a1c800204c497fd66f332669967de9021c8b","_cell_guid":"2bef9cbd-de61-4d16-9644-0054f9f49979"},"cell_type":"markdown","source":"> 実施予定日： 2019/07/14\n\n- 記入者: u110\n\n## このノートの目的\n\n- 元のkernelを読み解き、データ解析（可視化、表現するに有効な特徴量選択）についての体験し、理解を深める\n    - 元のkernel: https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n        - 取り扱うデータセット： 乳がん検査のデジタル画像を元に細胞の特徴を列挙したもの。"},{"metadata":{"_execution_state":"idle","_uuid":"bbc2abb85c63b8c357f5c2e0140105ed195f7fb4","_cell_guid":"b72f3424-c0c2-47e8-b18b-bc89253eab3f"},"cell_type":"markdown","source":"![](https://preview.ibb.co/bKsv9k/k.jpg)\n# INTRODUCTION\nIn this data analysis report, I usually focus on feature visualization and selection as a different from other kernels. Feature selection with correlation, univariate feature selection, recursive feature elimination, recursive feature elimination with cross validation and tree based feature selection methods are used with random forest classification. Apart from these, principle component analysis are used to observe number of components.\n\n**Enjoy your data analysis!!!**\n\n"},{"metadata":{"_uuid":"d92ab60363e823f9fe6d386242b47777d09ed868","_cell_guid":"c953148e-7a7a-4791-aef0-c62a2b159aec"},"cell_type":"markdown","source":"【訳】 序論\n\nこのデータ分析レポートでは、通常、私は他のカーネルとは違うものとして、特徴量の視覚化と選択に焦点を当てています。\n\n- 相関による特徴量選択\n- 単変量特徴量選択\n- 再帰的な特徴量の除去\n- クロスバリデーションによる再帰的な特徴量の削除\n- 木構造を用いた特徴選択方法\n\nこれらは、ランダムフォレストによる分類処理で良く使用されます。\n\nそれとは別に\n\n- Principle Component Analysis (PCA、主成分分析) \n\nを扱うことで特徴量の個数を検討することができます。\n\n**それではデータの分析をお楽しみください！！！**\n\n【メモ】 \n\n> このデータ分析レポートでは、通常、私は他のカーネルとは違うものとして、\n\n一般的なKernelはコンペのスコアを上げるためのTIPS、モデルの検証などがほとんどのため、\n今回のコンペのないデータセットと区別していると思います。\n\n"},{"metadata":{"_execution_state":"idle","_uuid":"c28fa7775a99901a882aee31e890ea99fe796d91","_cell_guid":"8d7fdae8-1d31-4873-a021-d553e2c4087c"},"cell_type":"markdown","source":"# Data Analysis"},{"metadata":{"_execution_state":"idle","_uuid":"d7dc365d2933b6675c57c98d438356e4cc1e6125","_cell_guid":"52942f7b-e58d-4275-86f0-ced1bcea06f9","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# 【訳】 このPython3の環境（kaggle kernelのnote）便利なライブラリがインストールされています。\n# 内容はこちらで定義されています。: https://github.com/kaggle/docker-python\n# 例えば以下の便利なパッケージがロードできます。\n\nimport numpy as np # linear algebra 数値計算を効率的に行うための拡張モジュール\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) データセットを操作するためのデータ構造と演算をサポートするモジュール\nimport seaborn as sns # data visualization library  可視化モジュール\nimport matplotlib.pyplot as plt # 可視化用関数\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport time\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"data.csv\n\n","name":"stdout"}]},{"metadata":{"_execution_state":"idle","_uuid":"4a65810840012b075b5a359994931bec8acf9ab0","_cell_guid":"c9bd4680-5a5d-4ce5-8b85-1820d2e478d0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/data.csv')","execution_count":2,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"81d7851cbda2bc774e989259005e98999b84c0b2","_cell_guid":"48131d2a-cb21-4c1f-8213-f0e78647287c"},"cell_type":"markdown","source":"Before making anything like feature selection,feature extraction and classification, firstly we start with basic data analysis. \nLets look at features of data."},{"metadata":{"_uuid":"b0e2293541a2bdf2b7f27f44ca1bdd6c60e628b2","_cell_guid":"09e461da-e2d7-4d3e-b854-d99cbd589274"},"cell_type":"markdown","source":"【訳】 特徴量選択、抽出、そして分類などを行う前にまず基本的なデータ分析から始めます。\nデータの特徴量を見てみましょう。"},{"metadata":{"_execution_state":"idle","_uuid":"3e01972c830afa1ce55025c0b7a202d4b204dd1d","_cell_guid":"d30f1486-bb97-40d7-9125-67e6f15286dc","trusted":true},"cell_type":"code","source":"data.head()  # head method show only first 5 rows","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"         id diagnosis     ...       fractal_dimension_worst  Unnamed: 32\n0    842302         M     ...                       0.11890          NaN\n1    842517         M     ...                       0.08902          NaN\n2  84300903         M     ...                       0.08758          NaN\n3  84348301         M     ...                       0.17300          NaN\n4  84358402         M     ...                       0.07678          NaN\n\n[5 rows x 33 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>diagnosis</th>\n      <th>radius_mean</th>\n      <th>texture_mean</th>\n      <th>perimeter_mean</th>\n      <th>area_mean</th>\n      <th>smoothness_mean</th>\n      <th>compactness_mean</th>\n      <th>concavity_mean</th>\n      <th>concave points_mean</th>\n      <th>symmetry_mean</th>\n      <th>fractal_dimension_mean</th>\n      <th>radius_se</th>\n      <th>texture_se</th>\n      <th>perimeter_se</th>\n      <th>area_se</th>\n      <th>smoothness_se</th>\n      <th>compactness_se</th>\n      <th>concavity_se</th>\n      <th>concave points_se</th>\n      <th>symmetry_se</th>\n      <th>fractal_dimension_se</th>\n      <th>radius_worst</th>\n      <th>texture_worst</th>\n      <th>perimeter_worst</th>\n      <th>area_worst</th>\n      <th>smoothness_worst</th>\n      <th>compactness_worst</th>\n      <th>concavity_worst</th>\n      <th>concave points_worst</th>\n      <th>symmetry_worst</th>\n      <th>fractal_dimension_worst</th>\n      <th>Unnamed: 32</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842302</td>\n      <td>M</td>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.3001</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>1.0950</td>\n      <td>0.9053</td>\n      <td>8.589</td>\n      <td>153.40</td>\n      <td>0.006399</td>\n      <td>0.04904</td>\n      <td>0.05373</td>\n      <td>0.01587</td>\n      <td>0.03003</td>\n      <td>0.006193</td>\n      <td>25.38</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.1622</td>\n      <td>0.6656</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842517</td>\n      <td>M</td>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.0869</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>0.5435</td>\n      <td>0.7339</td>\n      <td>3.398</td>\n      <td>74.08</td>\n      <td>0.005225</td>\n      <td>0.01308</td>\n      <td>0.01860</td>\n      <td>0.01340</td>\n      <td>0.01389</td>\n      <td>0.003532</td>\n      <td>24.99</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.1238</td>\n      <td>0.1866</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>84300903</td>\n      <td>M</td>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.1974</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>0.7456</td>\n      <td>0.7869</td>\n      <td>4.585</td>\n      <td>94.03</td>\n      <td>0.006150</td>\n      <td>0.04006</td>\n      <td>0.03832</td>\n      <td>0.02058</td>\n      <td>0.02250</td>\n      <td>0.004571</td>\n      <td>23.57</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.1444</td>\n      <td>0.4245</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>84348301</td>\n      <td>M</td>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.2414</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>0.4956</td>\n      <td>1.1560</td>\n      <td>3.445</td>\n      <td>27.23</td>\n      <td>0.009110</td>\n      <td>0.07458</td>\n      <td>0.05661</td>\n      <td>0.01867</td>\n      <td>0.05963</td>\n      <td>0.009208</td>\n      <td>14.91</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.2098</td>\n      <td>0.8663</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>84358402</td>\n      <td>M</td>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.1980</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>0.7572</td>\n      <td>0.7813</td>\n      <td>5.438</td>\n      <td>94.44</td>\n      <td>0.011490</td>\n      <td>0.02461</td>\n      <td>0.05688</td>\n      <td>0.01885</td>\n      <td>0.01756</td>\n      <td>0.005115</td>\n      <td>22.54</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.1374</td>\n      <td>0.2050</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_execution_state":"idle","_uuid":"16fbbd5b380761a5e22478133c1f8ead52ca7abb","_cell_guid":"d3b8329c-20eb-4f00-b900-ffc9278cd82a"},"cell_type":"markdown","source":"**There are 4 things that take my attention**\n1) There is an **id** that cannot be used for classificaiton \n2) **Diagnosis** is our class label\n3) **Unnamed: 32** feature includes NaN so we do not need it.\n4) I do not have any idea about other feature names actually I do not need because machine learning is awesome **:)**\n\nTherefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!"},{"metadata":{"_uuid":"3c7110e512a02f1461b6618aaba3406c1248a577","_cell_guid":"473c4a6a-2ef8-4e21-ae7f-dfe94e1fddc9"},"cell_type":"markdown","source":"【訳】 **私の注意を引く点が4つある**\n\n1）分類には使用できない ** id ** があります\n\n2） **Diagnosis(診断結果)** はクラスラベルです\n\n3） **Unnamed: 32** 列にはNaNが含まれているので、必要ありません。\n\n4） その他の特徴量の名称についてはわかりません。 実際には機械学習がすばらしいので、理解する必要はありません。 **:)** \n\nしたがって、これらの不要な特徴量を削除してください。 しかし、これは特徴量選択の作業ではないことを忘れないでください。 これはパブを眺めるようなものですが、まだドリンクを選んいるわけではないですよ!!!"},{"metadata":{"_uuid":"0750888e63ebd845e2a8777e4f47fd9624289631","_cell_guid":"ee26ce7b-6018-485c-84ea-c37461a7d8b2"},"cell_type":"markdown","source":"【メモ】\n\n4) について、実務においてはこのスタンスはちょっと悩ましいと思いました。\n\nこの後データの分布や変数間の相関を見て分析するフェーズにはいるはずで、\nデータの傾向を見つけたときに **そのデータの意味を知っているとその傾向について確信がもてるかもしれない** ので意味について知っておくのは大事と考えます。\nまた、データの意味と傾向に違和感などに気づくこともできます。\n（データの意味が間違っていたり、整形段階で誤った処理を行っているなど）\n\nただし、意味に捕らわれて時間がかかってしまったり、\n本来の目的を忘れて別の調査をしてしまうなどもやりがちなので、\nほどほどにして進めるのが良さそうです。"},{"metadata":{"_execution_state":"idle","_uuid":"54600377cdbec016505dcb970bb1988afbc260a2","_cell_guid":"60308baf-344a-41fb-8580-cef707ce5aa8","trusted":false,"collapsed":true},"cell_type":"code","source":"# feature names as a list\ncol = data.columns       # .columns gives columns names in data \nprint(col)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"94ea75618315ac7af54cf80a501c42b40e77ecbc","_cell_guid":"8764b4cf-9963-4c1a-b449-059de8153e4c","trusted":false,"collapsed":true},"cell_type":"code","source":"# y includes our labels and x includes our features\ny = data.diagnosis   # M or B \nlist = ['Unnamed: 32','id','diagnosis']   # 除外したい対象の列名\nx = data.drop(list,axis = 1 )   # listという配列をもとに除外\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"83a5e84835e8f52e091158cb40a12b92652cb771","_cell_guid":"399f6635-7f2e-4c6d-8548-0d97d9175eb4","trusted":false},"cell_type":"code","source":"# 個人的には .T関数で転置すると表示が縦になるので見やすい場合もあります。\n# x.head().T","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"71fecf26e957a2d670182d607aca5a7b92b4a3b6","_cell_guid":"31ec8d06-ea25-4b34-84ca-c322b3d8a10f","trusted":false,"collapsed":true},"cell_type":"code","source":"ax = sns.countplot(y,label=\"Count\")       # M = 212, B = 357\nB, M = y.value_counts()\nprint('Number of Benign: ',B)\nprint('Number of Malignant : ',M)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"af412eff1a8fa83f1f9fb6daaeac78cf498d589a","_cell_guid":"8c7d5b2b-445c-45ec-81da-5b3853617711"},"cell_type":"markdown","source":"Okey, now we have features but **what does they mean** or actually **how much do we need to know about these features**\nThe answer is that we do not need to know meaning of these features however in order to imagine in our mind we should know something like variance, standart deviation, number of sample (count) or max min values.\nThese type of information helps to understand about what is going on data. For example , the question is appeared in my mind the **area_mean** feature's max value is 2500 and **smoothness_mean** features' max 0.16340. Therefore **do we need standirdization or normalization before visualization, feature selection, feature extraction or classificaiton?** The answer is yes and no not surprising ha :) Anyway lets go step by step and start with visualization.  "},{"metadata":{"_uuid":"421a47125a822fa753229882b8347f3aaf97107f","_cell_guid":"b28c6694-71f9-4fa6-a460-1e662c1a8364"},"cell_type":"markdown","source":"【訳】　オッケー、この特徴量ですが、**それらの意味について** や **これらの特徴量についてどれだけ知る必要があるのでしょうか？**\n答えは、これらの意味を知る必要はありません。 \nしかし、データのイメージを掴むため分散、標準偏差、サンプル数（カウント）、または最大最小値については知っておいたほうが良いでしょう。 \nこれら代表値の情報は、データの状況を理解するのに役立ちます。 \nたとえば、**area_mean** の最大値は2500、**smoothness_mean**機能の最大値は0.16340です。 \nその場合、**可視化、特徴量選択、特徴量抽出または分類の前に、標準化または正規化が必要でしょうか？** \n答えは「イエス」であり、驚くべきことではありません。 :）\nとにかく一歩ずつ進めて、次に可視化の作業を始めましょう。"},{"metadata":{"_uuid":"4a29ef793cce648e462361ffeab247fa44db4c21","_cell_guid":"5b354adb-6651-4480-b143-511c68e82bd2"},"cell_type":"markdown","source":"【メモ】\nここでもここの特徴量について知る必要は無いという意見でしたが、特徴量を選ぶタイミングでは意味を知っておいたほうが良いと思っています。\nもしかしたら、今回のノートでは手法の紹介と割り切っているためかもしれません。参加者のみなさんと議論したいところ。"},{"metadata":{"_execution_state":"idle","_uuid":"7d909ed445dd83306413a72986cebc17d1814cc7","_cell_guid":"c92292c2-d999-42f3-8618-73b231c163e6","trusted":false,"collapsed":true},"cell_type":"code","source":"x.describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4162e457efbbf314d751bcf963220eaea5686049","_cell_guid":"b17ebf2c-e2c8-401d-9d76-4806f4470fd4","trusted":false},"cell_type":"code","source":"# これも転置させると全部見れる。\n# x.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"3edac4b24f82f00d32efe9d812aed40fb06fdbed","_cell_guid":"6179a010-0819-481e-8095-72ba1021fdcd"},"cell_type":"markdown","source":"# Visualization\nIn order to visualizate data we are going to use seaborn plots that is not used in other kernels to inform you and for diversity of plots. What I use in real life is mostly violin plot and swarm plot. Do not forget we are not selecting feature, we are trying to know data like looking at the drink list at the pub door."},{"metadata":{"_uuid":"303a09fa6bb5b93330a6c37ab45f75290185cc95","_cell_guid":"c8757045-c036-4c08-a692-5efe5ca8c175"},"cell_type":"markdown","source":"【訳】 可視化\n\nデータを視覚化するために、他のカーネルでは使用されていないseabornプロットを使用して、多数の描画方法について紹介します。 私がよく使っているのは、主にバイオリン図とスワームプロット（蜂群図）です。 我々は特徴量選択をしているわけでないことに注意してください、我々はパブのドアで飲み物のリストを眺めているように、データを知ろうとしているところです。"},{"metadata":{"_execution_state":"idle","_uuid":"d3d8066df83b9a30087610eed09782c1dec7c4cf","_cell_guid":"fa50c6cf-ccb6-49fa-b4bb-e7f14a3c4a09"},"cell_type":"markdown","source":"Before violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better."},{"metadata":{"_uuid":"8b498ab1adcde99ac91d739656d6d65a00eb9225","_cell_guid":"269b7e38-0314-48fe-bed5-a10c21b185ee"},"cell_type":"markdown","source":"【訳】バイオリン、スワームプロットを作成する前に、正規化(normalization)や標準化(standardization)が必要です。 特徴量の値の間の差はグラフで観察するのが非常に大きすぎるためです。\n私は3つのグループに特徴量をプロットし、各グループには10つの特徴量が含まれています。"},{"metadata":{"_uuid":"aa893d9cbe6019fd1a611958afeb2a9400747fbb","_cell_guid":"5c02425d-1dd9-4818-bccd-6a85bc6d128e"},"cell_type":"markdown","source":"【メモ】 正規化、標準化について\n\n単位や次元が違うことで、数量をそのまま比較することができないものを代表値などで割るなどして比較できるようにすること。\n\n以下では\n\n$$\nz = \\frac{X - \\mu}{\\sigma}\n$$\n\nという式で平均0標準偏差1の分布に変換しています。"},{"metadata":{"_execution_state":"idle","_uuid":"d640909614b5ff561e35b33e555458df70b22486","_cell_guid":"d58052d6-9e8c-46f4-a2d5-b9e82b247f27","trusted":false,"collapsed":true},"cell_type":"code","source":"# first ten features\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) / (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"6c3ce53008a590504b1afef35e5edff183ed0ce4","_cell_guid":"8eb078f8-63a8-405d-8cdb-10c9e8a06863"},"cell_type":"markdown","source":"Lets interpret the plot above together. For example, in **texture_mean** feature, median of the *Malignant* and *Benign* looks like separated so it can be good for classification. However, in **fractal_dimension_mean** feature,  median of the *Malignant* and *Benign* does not looks like separated so it does not gives good information for classification."},{"metadata":{"_uuid":"52d14e4fa42cf9b8455bb18f7849e9573fc3844e","_cell_guid":"dbb99898-b0de-464e-8dc3-8320cbc2e23d"},"cell_type":"markdown","source":"【訳】　上記のプロットを一緒に解釈しましょう。 たとえば、**texture_mean** では、 *悪性腫瘍(Malignant)* 　と　*良性腫瘍(Benign)*　の中央値が分離しているように見えるため、分類に適していると言えそうです。 しかし、 *fractal_dimension_mean* は、悪性および良性の中央値は分離されていないように見えるので、分類のための良い情報は得られていません。"},{"metadata":{"_uuid":"f796bae8b8805901aa6c55267828fc7b0a3fb159","_cell_guid":"06dd6c5b-d388-41f3-9f28-0f720f996d62"},"cell_type":"markdown","source":"【メモ】 上のコードは今までの中では長めのコードになるので、順を追って見てみます。"},{"metadata":{"_execution_state":"idle","_uuid":"0a18301387ce26b58a68e5a2d340b39e86c1f5e0","_cell_guid":"46ee71d3-93c1-4c2d-bb00-6995f7a1c816","trusted":false,"collapsed":true},"cell_type":"code","source":"data_dia = y  #  y = data.diagnosis\ndata = x\ndata_n_2 = (data - data.mean()) / (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\n\n\n# 列が横長になっているものを列名を持つ列を追加した上で縦長に整形(ここでは前半3行のデータのみ使って整形しています)\ndata = pd.melt(data.head(3),id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# データを縦長にもたせseabornのプロットに適した形に整形している。\ndata","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"d1c4e84c3d6bda4b9ff9e284d8c790dd46980c31","_cell_guid":"58f17ef1-6530-4db8-bcd9-32691363e8a9","trusted":false,"collapsed":true},"cell_type":"code","source":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64e307aa50995dafb2f07c4db9f6ddab3c6d7a56","_cell_guid":"8637e815-4d36-4f0c-ab6c-af71de4da61d","trusted":false,"collapsed":true},"cell_type":"code","source":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"80d14a320a514d9e727135db6d64155bea3ca35b","_cell_guid":"7e3412af-5856-4cb1-922d-6a3dd6c5f238"},"cell_type":"markdown","source":"Lets interpret one more thing about plot above, variable of **concavity_worst** and **concave point_worst** looks like similar but how can we decide whether they are correlated with each other or not.\n(Not always true but, basically if the features are correlated with each other we can drop one of them)"},{"metadata":{"_uuid":"eafdec6576f77e495ed1e583f421578f71ab6322","_cell_guid":"1565dfc7-01e2-44f7-93a6-c4abd34cb064"},"cell_type":"markdown","source":"【訳】 上のプロットについてもう一つ解釈すると、 **concavity_worst** と **convex_worst** は同様のように見えますが、どうやって相関しているかどうかを判断できえうしょうか。 （絶対ではないが、基本的に特徴量が相関が強い場合、その特徴量の1つを削除できます）\n\n【メモ】\n\n機械学習、統計モデルでは、各説明変数は独立変数であるという仮定を置くので、\n説明変数間で強い相関がある＝独立ではない　となるとそもそもの仮定が間違ってしまいます。\n強い相関のある特徴量を含んだままモデルの構築（パラメタ推定）を行うと、安定しなかったり、性能が悪くなってしまうので注意が必要です。\n詳しくは　「マルチコ」、「多重共線性」というワードで調べてみてください。"},{"metadata":{"_execution_state":"idle","_uuid":"f76a463fada43aa587f7bd035148698e71db6309","_cell_guid":"d6282b97-002d-4aeb-a09a-ce4ef138a1ca"},"cell_type":"markdown","source":"In order to compare two features deeper, lets use joint plot. Look at this in joint plot below, it is really correlated.\n Pearsonr value is correlation value and 1 is the highest. Therefore, 0.86 is looks enough to say that they are correlated. \nDo not forget, we are not choosing features yet, we are just looking to have an idea about them."},{"metadata":{"_uuid":"3bdc351f2b148e9f1ae65a7af546e122cafc2234","_cell_guid":"8af55f4f-d899-4042-b1a0-cb63ee82bc00"},"cell_type":"markdown","source":"【訳】 この2つの特徴量をより深く比較するために、ジョイントプロットを使用できます。 下のジョイントプロットを見てみるとやはり強い相関を示しています。 ピアソン相関係数は、1が最も高い。 したがって、0.86は十分相関があると言えます。 私たちはまだ特徴量選択をしていないことを忘れないでください。私たちは特徴量選択のためのアイデアを探している段階です。"},{"metadata":{"_uuid":"1d2fe41a8fdf41a1216d1350597819559a006910","_cell_guid":"5d8e8cfe-8807-4596-83a0-88f9c2d050e0","trusted":false,"collapsed":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')  # 最新でパッチがあたるので今は無視 https://github.com/mwaskom/seaborn/issues/1392\n\nsns.jointplot(x=x.loc[:,'concavity_worst'], y=x.loc[:,'concave points_worst'], kind=\"reg\", color=\"#ce1414\");","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"43de55ee2437101c0450d989216a426d6f1f30c7","_cell_guid":"008330ba-393e-4d31-b086-b90332a613c5"},"cell_type":"markdown","source":"What about three or more feauture comparision ? For this purpose we can use pair grid plot. Also it seems very cool :)\nAnd we discover one more thing **radius_worst**, **perimeter_worst** and **area_worst** are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection."},{"metadata":{"_uuid":"f526a15e68e441679ec0d43cdbb4875dce296e4f","_cell_guid":"c00aa339-2309-42c8-9ca8-049a61ce3f20"},"cell_type":"markdown","source":"【訳】 3つ以上の特徴量の比較はにはペアグリッドプロットを使用することができます。 それは非常にクールだ:)\n\n私たちは新たに **radius_worst**、 **perimeter_worst** と **area_worst** について相関があることを発見し、\nペアグリッドプロットとでも確認できます。 \n我々はこれらの発見を特徴選択のために使用します。"},{"metadata":{"_execution_state":"idle","_uuid":"381ecb55ced22383c96320ced2299f5da37ce4b6","_cell_guid":"3bda33fe-daf9-4f74-acbc-d9d3c8fc83d9","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.set(style=\"white\")\ndf = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.PairGrid(df, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")  # 下部を2変量のカーネル密度推定値の描画\ng.map_upper(plt.scatter)  # 上部を散布図行列の描画\ng.map_diag(sns.kdeplot, lw=3)  # 対角要素を　1変量のカーネル密度推定値の描画","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"f8355f83ac16e414e3b88e67b77d33ef31c3574d","_cell_guid":"c9ef0921-19bc-4d40-aa72-0bc2333bd4b4"},"cell_type":"markdown","source":"Up to this point, we make some comments and discoveries on data already. If you like what we did, I am sure swarm plot will open the pub's door :) "},{"metadata":{"_execution_state":"idle","_uuid":"c3807ef7f6e17b33ae383349bdee7ebfced2a847","_cell_guid":"03abd05a-d67a-4bfc-b951-6394de8c6fc9"},"cell_type":"markdown","source":"In swarm plot, I will do three part like violin plot not to make plot very complex appearance"},{"metadata":{"_uuid":"3cc7eebc00e161b4f23cd3c372c6e90a792941e7","_cell_guid":"0f7b2ee7-b154-457b-b370-a2eb8d50939c"},"cell_type":"markdown","source":"【訳】　ここまで、データに関するコメントや発見を行ってきました。 私たちがしたことが好きなら、私はスウォームプロットがパブのドアを開けると確信しています:)\n\nスウォームプロット（蜂群図）でも、繁雑な描画にならないように、バイオリンプロット同様に3つに分けて行います。"},{"metadata":{"_execution_state":"idle","_uuid":"85a2413b70c1b3d69f26a2c122c22d55f930e774","_cell_guid":"ef378d49-8aed-4b9e-96e8-e7d2458fdd89","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.set(style=\"whitegrid\", palette=\"muted\")\ndata_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) / (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"75dfd5e9e50adceb1d42dd000ce779e79b069cce","_cell_guid":"428c75b6-b5d0-47e3-a568-17b5d1896c0c","trusted":false,"collapsed":true},"cell_type":"code","source":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"209e9e9120d6e889696d2d1190e663b5c1885a82","_cell_guid":"ee64bbc8-0431-482a-b08f-cdca43e41390","trusted":false,"collapsed":true},"cell_type":"code","source":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\ntoc = time.time()\nplt.xticks(rotation=90)\nprint(\"swarm plot time: \", toc-tic ,\" s\")\n# 処理時間を表示\nprint(\"swarm plot time: \", toc-tic ,\" s\")","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"d23e2f9b040a92feb0d7ceb8e01e74c758f5dbc3","_cell_guid":"5c9efa5e-4938-477e-ab9a-4b084cc0b870"},"cell_type":"markdown","source":"They looks cool right. And you can see variance more clear. Let me ask you a question, **in these three plots which feature looks like more clear in terms of classification.** In my opinion **area_worst** in last swarm plot looks like malignant and benign are seprated not totaly but mostly. Hovewer, **smoothness_se** in swarm plot 2 looks like malignant and benign are mixed so it is hard to classfy while using this feature."},{"metadata":{"_uuid":"494c578ab58595c94d34d07d16f23ef3857d8a7a","_cell_guid":"a846707e-fec8-43ee-81fa-742dd22ba2e3"},"cell_type":"markdown","source":"【訳】 クールだね。 そして、あなたは分散をより明確に見えているでしょう。 質問をしてみましょう。 **これらの3つのプロットの中で、分類を行う上で有効な特徴量はどれでしょう?** 私の意見では最後の描画にある **area_worst** はほぼうまく分離しているように見えます。 一方二番目の描画に含まれている **smoothness_se**　については良性、悪性が混じっているため、この特徴量を利用してもクラス分けは難しいでしょう。"},{"metadata":{"_execution_state":"idle","_uuid":"b46f98eb7ca8d36dc7bf1516895599524bab694d","_cell_guid":"c4c68f34-e876-4e5a-a4a7-09c07381425a"},"cell_type":"markdown","source":"**What if we want to observe all correlation between features?** Yes, you are right. The answer is heatmap that is old but powerful plot method."},{"metadata":{"_uuid":"82cf1de6189ba5ba61df538b672b47badfbcc811","_cell_guid":"fd079bd8-bb29-4048-a0ff-a8e0f77124e1"},"cell_type":"markdown","source":"【訳】　すべての特徴量について相関を見たい場合、どうすればよいでしょう？ はい、正解です。答えはヒートマップを使います。これは昔からある強力な描画方法です。"},{"metadata":{"_execution_state":"idle","_uuid":"0eeb70ddffc8ac332ee076f2f6b2833a6ffddd2d","_cell_guid":"9e1e7d8a-bbf2-4aab-90e7-78d4c4ccf416","trusted":false,"collapsed":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"b4e3afecb204a8262330d22e4539554b2af7975a","_cell_guid":"8ee2e02b-9fc6-42f7-8a83-f6dd77df6c11"},"cell_type":"markdown","source":"Well, finaly we are in the pub and lets choose our drinks at feature selection part while using heatmap(correlation matrix)."},{"metadata":{"_uuid":"d31b1a0be321fa57efde835662e7e873332809da","_cell_guid":"4aca38c2-1a08-4171-b9fa-e16851539c35"},"cell_type":"markdown","source":"【訳】 ここでやっと私たちはパブに入ったので、相関行列のヒートマップを使い、後半の特徴量選択パートで飲み物を選びましょう。"},{"metadata":{"_execution_state":"idle","_uuid":"84b145dd0c13a3a0d4dd4f9b9fd1bd782e11fcf8","_cell_guid":"6786734a-40a9-46b6-a13a-97ee9c569636"},"cell_type":"markdown","source":"# Feature Selection and Random Forest Classification\nToday our purpuse is to try new cocktails. For example, we are finaly in the pub and we want to drink different tastes. Therefore, we need to compare ingredients of drinks. If one of them includes lemon, after drinking it we need to eliminate other drinks which includes lemon so as to experience very different tastes."},{"metadata":{"_uuid":"053ff2c9566feef3f0ec83932cf1b104ff2b256a","_cell_guid":"8c8b6ff5-7768-4d2b-9e79-2aff1e6b374e"},"cell_type":"markdown","source":"【訳】目的は新しいカクテルを作ることだ。  \n例えば、我々はとうとうパブに入って、いろいろな味を飲み比べている。  \nしたがって、飲み物の成分を比べる必要がある。  \nもしレモンが入っているものを飲んだ後は、もっと違う味を知るためにレモンの入っていない飲み物を飲みましょう。"},{"metadata":{"_execution_state":"idle","_uuid":"a042df90ef7138d6f101463e93936119176bdc0d","_cell_guid":"c7b2df4e-270e-4c94-8789-177f5e90ac46"},"cell_type":"markdown","source":"In this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict. "},{"metadata":{"_uuid":"d62354fb67ce6e7ef45e85b74c86613afe17fd68","_cell_guid":"b7d5ae53-9fcf-4302-a99c-892e951f18ca"},"cell_type":"markdown","source":"【訳】ここでは以下の異なる特徴量選択手法を見てみる。\n* 単変量特徴量選択\n* 再帰的特徴量除去 (RFE)\n* クロスバリデーションと共にRFE (CVRFE)\n* 木構造特徴量選択  \nモデルの訓練と推測にはランダムフォレストを使う。"},{"metadata":{"_execution_state":"idle","_uuid":"39003c7b75f265bf0826f407433e65923c4dd017","_cell_guid":"94d217e3-b2b3-4016-b72e-f8d521d17af7"},"cell_type":"markdown","source":"## 1) Feature selection with correlation and random forest classification"},{"metadata":{"_execution_state":"idle","_uuid":"1e6ef08c98cb4bf0dedf275e4c08fae743bb3801","_cell_guid":"785bd27a-30d9-4e08-a864-cde7e5630aad"},"cell_type":"markdown","source":"As it can be seen in map heat figure **radius_mean, perimeter_mean and area_mean** are correlated with each other so we will use only **area_mean**. If you ask how i choose **area_mean** as a feature to use, well actually there is no correct answer, I just look at swarm plots and **area_mean** looks like clear for me but we cannot make exact separation among other correlated features without trying. So lets find other correlated features and look accuracy with random forest classifier. "},{"metadata":{"_uuid":"9dc385ea1ffe004c801277807a7ea36716501038","_cell_guid":"b100e5bf-3ac4-422f-a224-df3c990ec1fd"},"cell_type":"markdown","source":"【訳】ヒートマップで見たように、**radius_mean**,**perimeter_mean**,**area_mean**はそれぞれ相関していたため、**area_mean**のみを使うとしよう。  \nなぜ**area_mean**を使うのかって？特に理由はないが、強いて言えば**area_mean**の swarm plot がよく分離しているように見えたからだ。  \nだが、他の相関のある特徴量はまだ除去していない。  \nでは他の相関のある特徴量を探したのち、ランダムフォレストの精度を見てみよう。"},{"metadata":{"_uuid":"f3b90b57e67e83f52d58d126a3c10d4cc4f534e8","_cell_guid":"e12e5dbd-e93b-4751-9d5c-6b52843c57f7"},"cell_type":"markdown","source":"【コメント】Random Forest とは、複数の決定木を弱学習器としてそれぞれ異なるデータで学習させ、各弱学習器の予測結果の平均を出力するモデルのこと。  \n決定木の過学習をしやすいというデメリットを解決するための手段で、特に分類器として安定して高い精度が期待できる。"},{"metadata":{"_execution_state":"idle","_uuid":"acde9c0b406d72122473f8292d641a9fcb8a8682","_cell_guid":"eea2971b-b703-4e1b-b048-128501506f33"},"cell_type":"markdown","source":"**Compactness_mean, concavity_mean and concave points_mean** are correlated with each other.Therefore I only choose **concavity_mean**. Apart from these, **radius_se, perimeter_se and area_se** are correlated and I only use **area_se**.  **radius_worst, perimeter_worst and area_worst** are correlated so I use **area_worst**.  **Compactness_worst, concavity_worst and concave points_worst** so I use **concavity_worst**.  **Compactness_se, concavity_se and concave points_se** so I use **concavity_se**. **texture_mean and texture_worst are correlated** and I use **texture_mean**. **area_worst and area_mean** are correlated, I use **area_mean**.\n\n\n"},{"metadata":{"_uuid":"092496137876f1f293f9b5d7b5a17e004fb2d338","_cell_guid":"2bdc9e5c-c343-4877-a4fd-c9c7cd0a86c8"},"cell_type":"markdown","source":"【訳】相関のある特徴量と選択した特徴量の一覧  \n* **compactness_mean**, **concavity_mean**, **concave points_mean** -> **concavity_mean**\n* **radius_se**, **perimeter_se**, **area_se** -> **area_se**\n* **radius_worst**, **perimeter_worst**, **area_worst** -> **area_worst**\n* **compactness_worst**, **concavity_worst**, **concave points_worst** -> **conavity_worst**\n* **concavity_se**, **concave points_se** -> **concavity_se**\n* **texture_mean**, **texture_worst** -> **texture_mean**\n* **area_worst**, **area_mean** -> **area_mean**\n\n【コメント】根拠が明記されていないが、四捨五入して0.9以下になるものに絞ったように見える。  \nただ、最終的に残すことにした特徴量を選んだ理由は不明。"},{"metadata":{"_execution_state":"idle","_uuid":"117f3e858e806f3f26a68dadf3fc89d471010156","_cell_guid":"ef8d06df-bfcc-4e9a-a3ba-5016ec0c5bd5","trusted":false,"collapsed":true},"cell_type":"code","source":"# 先ほど選んだ相関のある特徴量のうち、使用しないものをリストアップしている\ndrop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later  # xそのものはあとで使うので、新しい変数名にしている\nx_1.head()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"1ab3852ed7fbeba8718e6722e8a40521033bdf29","_cell_guid":"6de99062-7a5a-4b70-879c-54d6c8a4a7e2"},"cell_type":"markdown","source":"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it."},{"metadata":{"_uuid":"254c9413a36cbcb1f058584bc4b38f7c736ab782","_cell_guid":"c21e9b3e-305d-40e5-bc43-bf71c2312404"},"cell_type":"markdown","source":"【訳】相関のある特徴量を落としたので、下の相関行列をみて分かるように、もう相関のある特徴量は残っていない。  \n確かに、0.9以上の相関値は残っているが、これを落とさないことで何が起きるのか見てみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"eec5424039036e1af43ba0795b76393805308f97","_cell_guid":"733f0784-4a3f-410c-a220-f98591825f2e","trusted":false,"collapsed":true},"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax,)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"0eaf4ae8e33c2d6352a862953c1fe5eecf46ed27","_cell_guid":"f6551fe9-f3a8-4738-ace3-b0427be4aeb4"},"cell_type":"markdown","source":"Well, we choose our features but **did we choose correctly ?** Lets use random forest and find accuracy according to chosen features."},{"metadata":{"_uuid":"9373e7aec89b975204932146967cc901f67b399d","_cell_guid":"7190bf40-2e63-490c-91fd-4d984b9daca7"},"cell_type":"markdown","source":"【訳】さて、特徴量の選択をしたわけだが、**果たして正しい選択をしていたのだろうか？**  \nランダムフォレストを使って選択した特徴量について精度を出してみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"c7a6af60a44959f81593d788934a49c9259d8b43","_cell_guid":"111af932-96f8-4105-8deb-ba1172edd203","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 % # 7:3 で train/test に分割\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default) # 弱学習器が10個(デフォルト値)のランダムフォレスト分類器\nclf_rf = RandomForestClassifier(random_state=43) # random_stateを妙な値に設定している。怪しい。\nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\n# 混同行列を出力する関数\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d995a75866e42438836996a6a8ff1a2a40b76a3f","_cell_guid":"a6299d2f-37e7-41bd-965a-400e2254d24b","trusted":false,"collapsed":true},"cell_type":"code","source":"# 追記\n# 相関のある特徴量を落とさないとどのような結果が出るのか実験\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\n# x_1 ではなく　x　を使用している\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\", )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"21cda299619940f7b22acc9a804ee56bff71d3e7","_cell_guid":"1503384d-ca2b-4b52-82b5-f1131b014269"},"cell_type":"markdown","source":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. \nNow lets see other feature selection methods to find better results."},{"metadata":{"_uuid":"b50da988bd7146850cd26262764fef03568c743f","_cell_guid":"0a8dcf93-6832-43dd-848f-e020ae90932c"},"cell_type":"markdown","source":"【訳】精度は約 95% であった。\nまた、混同行列を見ると、少しの間違った推測が発生している。  \nさて、他の特徴量選択手法でもっといい結果を模索してみよう。\n\n 【コメント】全特徴量を使った時の結果94.7% -> 95.3%  \n random_stateを変えたら逆転もするレベル。  \n ただし、大事なのは悪化していないということ。"},{"metadata":{"_execution_state":"idle","_uuid":"decd86422aee506b061c905e8573abb3612734e4","_cell_guid":"3eed9ac3-e601-4e16-85bc-26a1a6fff850"},"cell_type":"markdown","source":"## 2) Univariate feature selection and random forest classification\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.\n<http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest>"},{"metadata":{"_uuid":"09cfb0ab5213ca42d04e4c092d2fafcdae1b31dc","_cell_guid":"6d505d8f-4bfb-4086-b0db-834f5c09fd9f"},"cell_type":"markdown","source":"【訳】単変量特徴量選択では、SelectKBest を利用する。  \nこれはk個の最高スコアを出す特徴量以外を除去する方法である。"},{"metadata":{"_execution_state":"idle","_uuid":"f681583a7b20e4fb86e557b910021a263573cf18","_cell_guid":"f053659d-9dfe-4858-a220-ef327df3bc36"},"cell_type":"markdown","source":"In this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and find best 5 features."},{"metadata":{"_uuid":"6dfdd628315635e97ede1cb3db319e0b6b9b1963","_cell_guid":"1a39423e-a693-4f31-a480-a984146bc0cb"},"cell_type":"markdown","source":"【訳】この手法では、いくつの特徴量を選択するかを決めなければいけない。  \n例えば5 か 10 か 15 か？  \n答えはやってみるか直感的に決めるしかない。  \n全ての組み合わせを試しはしないが、とりあえずk = 5を選択してみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"8159f9efb106f1219dc4e8c2a340399b88f224d8","collapsed":true,"_cell_guid":"4f43c8bd-48f7-4ed9-aa2d-6aa8a29c0c58","trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\n# カイ2乗の値でスコアリング, 小さいほうが類似度が高いはず\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"d9dcd1495cbf33c190a0d1211df4bac5e79bc4e5","_cell_guid":"c9684618-06fe-4b0a-835f-ceea46da397c","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"351dbc0d0874665f5c1e92065012e5d2f6323914","_cell_guid":"2c865015-0db0-4947-82ac-d7847957ddcc","trusted":false,"collapsed":true},"cell_type":"code","source":"# 追記 対応関係がわかりにくいので、特徴量名とスコアを併記\npd.DataFrame(select_feature.scores_, index=x_train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"0f46ea1a9b1282a377549406d1c5e093380954b6","_cell_guid":"b0c426ef-3072-4f6e-bf24-b5d927a98316"},"cell_type":"markdown","source":"Best 5 feature to classify is that **area_mean, area_se, texture_mean, concavity_worst and concavity_mean**. So lets se what happens if we use only these best scored 5 feature."},{"metadata":{"_uuid":"e8bc04c2981e7c8d61a7fc84e8a98e693808689c","_cell_guid":"7e5b19dd-df41-4732-a990-e0625f92e44e"},"cell_type":"markdown","source":"【訳】分類のための特徴量のTop 5は以下の5つだった。\n* **area_mean**\n* **area_se**\n* **texture_mean**\n* **concavity_worst**\n* **concavity_mean**  \nではこの5つの特徴量のみを使うとどうなるか見てみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"9a2bd21537f7c600f3c9baaf833c001084d6ba00","_cell_guid":"efc70e04-bc9c-4f93-bcd3-b1d7160d0d5c","trusted":false,"collapsed":true},"cell_type":"code","source":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\",)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"575005da62c41d12bbb3999b3e26148e12930ce3","_cell_guid":"d8888dc1-b50b-46b4-b202-4e33c2630406"},"cell_type":"markdown","source":"Accuracy is almost 96% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar.\nNow lets see other feature selection methods to find better results."},{"metadata":{"_uuid":"ae493a7d853897f5f3bfdb3cac887d5a609d11da","_cell_guid":"f360d9c9-ae83-4074-83f0-246dceb41f34"},"cell_type":"markdown","source":"【訳】精度は約 96%、混同行列からいくつか間違いをしていることが分かる。  \n現状を整理する、まず相関行列から特徴量を選択して、次にselectkBest手法から特徴量を選択した。  \nにも関わらず、selectkBestで5この特徴量を選択しても同じような精度となった。  \nさて、他の特徴量選択手法も見てみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"7a3c3050dd9d694e52962c7c712b1ea16aab6fdf","_cell_guid":"702ad2b3-5b12-4d15-93b1-e7d62dfd1040"},"cell_type":"markdown","source":"## 3) Recursive feature elimination (RFE) with random forest\n<http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html>\nBasically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features"},{"metadata":{"_uuid":"9abc49a01b5786c55d76e9480726c58d90bf7fa7","_cell_guid":"8a8eda31-dd10-48db-beac-589cd3b35ddb"},"cell_type":"markdown","source":" 【訳】基本的に、これはある分類手法(今回はランダムフォレスト)を使用し、それぞれの特徴量に重みを付与する。  \nその重みの絶対値がもっとも小さいものを現在の特徴量セットから刈り込む。  \nこの手続きを再帰的に、欲しい特徴量数になるまで繰り返す。"},{"metadata":{"_execution_state":"idle","_uuid":"3ea45c46bb231c767160fe13ad3b21a70f0d0375","_cell_guid":"8a34a801-c568-4598-8a07-85fc20ad0386"},"cell_type":"markdown","source":"Like previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method."},{"metadata":{"_uuid":"01f6e7bf2a324bf6cb9c7e9501d504a44696ec63","_cell_guid":"6a3d7c3d-79e3-4504-8ac1-9df2f5b5a712"},"cell_type":"markdown","source":"【訳】単変量特徴量選択と同様に、5個の特徴量を使うこととしよう。  \nではどの5個の特徴量を使うのか？  \nRFE手法で選ぼう。  "},{"metadata":{"_execution_state":"idle","_uuid":"c384a5240d1c1e9e2a6750e5d218dadaf24d2035","collapsed":true,"_cell_guid":"8df88bb5-8003-4696-9efe-63ebf8d609a5","trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()\n# step=1 は毎回1つずつ特徴量を削るということ\nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"29ba35a98954d0ae686ce46295179d1f1a27b74c","_cell_guid":"51d63d0b-4e00-4dc1-816c-809287b60806","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"ce670f778a661e8ddc3b7b21a43ccb48a551581a","_cell_guid":"92aa6013-3e16-4005-ab1b-b7ce53e78bd3"},"cell_type":"markdown","source":"Chosen 5 best features by rfe is **texture_mean, area_mean, concavity_mean, area_se, concavity_worst**. They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is **5**. Maybe if we use best 2 or best 15 feature we will have better accuracy. Therefore lets see how many feature we need to use with rfecv method."},{"metadata":{"_uuid":"d56f5d92495d7bad4ab62368604862a23595cd64","_cell_guid":"d78b661d-361a-43cb-aa11-7117bdcd1e04"},"cell_type":"markdown","source":"【訳】選んだ特徴量\n* **area_mean**\n* **area_se**\n* **texture_mean**\n* **concavity_worst**\n* **concavity_mean**  \nselectkBestと全く同じ結果だった。  \n再び精度計算する必要はないだろう。  \n端的に言うと、selectkBestとRFE、どちらでも良い特徴量選択ができる。  \nここで問題となるのは、さあどちらの手法でも5個の最適な特徴量が得られたが、なぜ**5**であるのか？  \n2とか15個の特徴量を選べばもっと良い精度がでるのではないか？  \nそれでは何この特徴量が必要なのか、RFECV手法を使ってみよう。"},{"metadata":{"_uuid":"1235747c3a3e087565a600d2d6458a932ce2db0a","_cell_guid":"3255c222-9bb4-47cb-92f7-75e5a50fe367"},"cell_type":"markdown","source":"【コメント】Cross Validataion(交差検証)とは、訓練データを任意数nのグループに分割し、それぞれのグループを検証データとして性能測定した結果の平均値を最終的な性能とする評価方法のこと。  \n全訓練データを使用して性能評価をするため、信頼の高い評価が期待できる。"},{"metadata":{"_execution_state":"idle","_uuid":"42a8c3f2ef0e5978b620eea737e6e234dc79cfe8","_cell_guid":"22a4f840-2a37-4047-9804-129e7f68f74a"},"cell_type":"markdown","source":"## 4) Recursive feature elimination with cross validation and random forest classification\n<http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html>\nNow we will not only **find best features** but we also find **how many features do we need** for best accuracy."},{"metadata":{"_uuid":"e4cb8da48cdf1fc8b5e13988518d217391561634","_cell_guid":"35c4b0aa-31c0-461a-9620-1fb4474d4a2f"},"cell_type":"markdown","source":"【訳】それでは、最高精度を達成するためには**最適な特徴量を探す**だけでなく、**何個の特徴量が必要か**を探してみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"0d7803966979745a8bdbdbc44a1927558485640a","_cell_guid":"7a5d4d69-7734-4465-89cc-f46b4af4c548","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \n# cv=5 は5個に分けてCVするということ\nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"cef5972f5e9fa830e92bae00ca5dd7d2b0ac8c58","_cell_guid":"be0f7ce5-55c0-48c9-a125-616750f13943"},"cell_type":"markdown","source":"Finally, we find best 11 features that are **texture_mean, area_mean, concavity_mean, texture_se, area_se, concavity_se, symmetry_se, smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst** for best classification. Lets look at best accuracy with plot.\n"},{"metadata":{"_uuid":"929c8c28942766a291fa647bd6e949c1d8cd4be6","_cell_guid":"2138d8de-7992-4b52-885a-c973d0a161c1"},"cell_type":"markdown","source":"【訳】最終的に以下の11個の特徴量が最適であるという結果になった。  \n* **texture_mean**\n* **area_mean**\n* **concavity_mean**\n* **texture_se**\n* **area_se**\n* **concavity_se**\n* **symmetry_se**\n* **smoothness_worst**\n* **concavity_worst**\n* **symmetry_worst**\n* **fractal_dimension_worst**\n\nでは最適精度をプロットと共に見てみよう。"},{"metadata":{"_execution_state":"idle","_uuid":"f362bfa341032f2bb1bacc1c50675a1916f5c536","_cell_guid":"5b69144b-72e4-4ac3-b8a8-c9ebbf8ffa3b","trusted":false,"collapsed":true},"cell_type":"code","source":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a39fe9a7684c4bfbcd31a7fccb774593aba9c45a","_cell_guid":"aeae105b-d708-44c5-8566-14c15f78d9bf"},"cell_type":"markdown","source":"【コメント】今回の最適特徴量数は11だったが、プロットのガタつき具合から信頼度は低そう。  \n8を超えたくらいから緩やかに減少傾向があるように見受けられるので、10付近の値を使うのは理にかなっている。  \n余裕があれば複数回別シードでスキャンしてみても良いかもしれない。"},{"metadata":{"_execution_state":"idle","_uuid":"f071ec67bd63d5c458c2cb6303fe6f54458db57b","_cell_guid":"580f19b6-1182-43d5-932c-21e2fb5bb2d9"},"cell_type":"markdown","source":"Lets look at what we did up to this point. Lets accept that guys this data is very easy to classification. However, our first purpose is actually not finding good accuracy. Our purpose is learning how to make **feature selection and understanding data.** Then last make our last feature selection method."},{"metadata":{"_uuid":"a40d338800a34f0d2e0830811169a92da682c651","_cell_guid":"575950f0-b3db-4245-bffc-943ed13155db"},"cell_type":"markdown","source":"【訳】現状を整理する。  \nこのデータがとても分類が簡単であることは受け入れよう。  \nしかし、当初の目的は良い精度を追い求めることではない。  \n目的はどのように**特徴量選択とデータの理解**をするのかを学ぶことである。  \nでは最後に、最後の特徴量選択手法を行う。"},{"metadata":{"_execution_state":"idle","_uuid":"8bc3105398fc618e19deec4de957950cfb45c054","_cell_guid":"2637e8bc-d986-41c0-acef-ce76afc4c350"},"cell_type":"markdown","source":"## 5) Tree based feature selection and random forest classification\n<http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html>\nIn random forest classification method there is a **feature_importances_** attributes that is the feature importances (the higher, the more important the feature). **!!! To use feature_importance method, in training data there should not be correlated features. Random forest choose randomly at each iteration, therefore sequence of feature importance list can change.**\n"},{"metadata":{"_uuid":"aed721adcb8c7dd1065013ad51d34657ce0953c7","_cell_guid":"58d30b85-2694-4e28-90e3-8f5244c31e4e"},"cell_type":"markdown","source":"【訳】ランダムフォレストには**featureimportances**という属性があり、特徴量の重要性を意味する。(高いほどその特徴量が重要である。)  \n**注意** この手法を使う時は、訓練データの中に相関のある特徴量を入れてはいけない。  \nランダムフォレストは各イテレーションでランダムに選択を行うため、特徴量の重要性リストは変わることがある。"},{"metadata":{"_execution_state":"idle","_uuid":"31d4b248f723930ff7120ffaff2c260f07e3f0fc","scrolled":false,"_cell_guid":"df8abc8d-3279-4c31-a6b6-e4f272ca0b47","trusted":false,"collapsed":true},"cell_type":"code","source":"clf_rf_5 = RandomForestClassifier()      \nclr_rf_5 = clf_rf_5.fit(x_train,y_train)\n# RandomForestClassifier 全体の特徴量重要度\nimportances = clr_rf_5.feature_importances_\n# RandomForestClassifier.estimators_ に DicisionTreeClassifier のリストが入っている\n# その中のfeature_importances_ 属性のstdを取る\nstd = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(x_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(x_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"760b045b33388f6fb7b53acdf931e8204eea80cd","_cell_guid":"00008678-9012-4e50-aaab-010b3353ac98"},"cell_type":"markdown","source":"As you can seen in plot above, after 5 best features importance of features decrease. Therefore we can focus these 5 features. As I sad before, I give importance to understand features and find best of them. "},{"metadata":{"_uuid":"a8d245915f4f3a5e5c96b943c78814e4d7445a36","_cell_guid":"a3839e91-a1e8-4d51-8c2b-da8faa879a1a"},"cell_type":"markdown","source":" 【訳】上のプロットから分かるように、5個の最適特徴量以降は特徴量の重要度は減少していく。  \nしたがってこれらの5個の特徴量に目を向けることができる。  \n前にも言ったが、ここでは特徴量の理解と最適な特徴量の探索方法についての重要性を提示しているのだ。  \n【コメント】マイナスの値はないはず。エラーバーはstdなので、大きい外れ値があるようだ。"},{"metadata":{"_execution_state":"idle","_uuid":"c3fd1de4be5be26252a4105501a217a026d116b1","_cell_guid":"21ef3e97-1eba-4f30-9714-cc45a3c1a594"},"cell_type":"markdown","source":"# Feature Extraction\n<http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html>\nWe will use principle component analysis (PCA) for feature extraction. Before PCA, we need to normalize data for better performance of PCA.\n "},{"metadata":{"_uuid":"6fca5064dfc5417920a5f2a6d731a06d5e655b91","_cell_guid":"9ac38885-f4ec-4154-8562-4d161643ee5d"},"cell_type":"markdown","source":"【訳】特徴量抽出のため、主成分分析(PCA)を使う。  \nPCAが良い結果を出すために、PCAの前にデータの正規化をする必要がある。"},{"metadata":{"_uuid":"6391d94d790add9efeffdc574b3390ce51e377c5","_cell_guid":"d1ce1b2e-0adf-44aa-ba08-4a76b9163127"},"cell_type":"markdown","source":"【コメント】PCAは多変量解析において特徴量の次元を圧縮する手法。  "},{"metadata":{"_execution_state":"idle","_uuid":"cf72d82fea5d8330db8ec324fb18abc6e969bac6","_cell_guid":"aa440ca8-8282-4cc3-9683-d5ebdd992140","trusted":false,"collapsed":true},"cell_type":"code","source":"# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n#normalization\nx_train_N = (x_train-x_train.mean())/(x_train.max()-x_train.min())\nx_test_N = (x_test-x_test.mean())/(x_test.max()-x_test.min())\n\nfrom sklearn.decomposition import PCA\npca = PCA()\npca.fit(x_train_N)\n\n# plt.figure(1, figsize=(14, 13))\nplt.figure(1, figsize=(10, 7))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_ratio_')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"a50cb8de4ec6e2e8727a2b3b021ee42c4ce21b29","_cell_guid":"264e1dba-1f40-4a28-bd4b-5261a7df185b"},"cell_type":"markdown","source":"According to variance ration, 3 component can be chosen."},{"metadata":{"_uuid":"b9c2ff5df29e2ad947d7be7aaea21630852d1117","_cell_guid":"c6caefb6-1a81-4ac6-8dc3-02076ad6bb3c"},"cell_type":"markdown","source":"【訳】分散比によって、3成分が選ばれる。  \n【コメント】説明が少し雑なので補足。  \n主成分分析においては、圧縮された特徴量で全特徴量の分散の何%を表現できるかを指標とする。  \n一般的に95%程度の分散が表現できれば良いとされているため、3成分が必要十分と判断された(のだろう)。"},{"metadata":{"_execution_state":"idle","_uuid":"5e0f4ba2b385fe9312ced2455eed4fb87d39a0b8","_cell_guid":"224c9c75-256d-4650-af3e-c4c39b565661"},"cell_type":"markdown","source":"# Conclusion\nShortly, I tried to show importance of feature selection and data visualization. \nDefault data includes 33 feature but after feature selection we drop this number from 33 to 5 with accuracy 95%. In this kernel we just tried basic things, I am sure with these data visualization and feature selection methods, you can easily ecxeed the % 95 accuracy. Maybe you can use other classification methods.\n### I hope you enjoy in this kernel\n## If you have any question or advise, I will be apreciate to listen them ..."},{"metadata":{"_uuid":"a6be46514913c01ebf58a530f529869b05b71582","_cell_guid":"17976dd8-4927-4f0b-9f6a-7a5f77d49cc0"},"cell_type":"markdown","source":"【訳】端的に言うと、ここでは特徴量選択とデータ可視化の重要性をお見せしようとしてきた。  \n元のデータは33の特徴量を含むが、特徴量選択後には33から5まで特徴量数を落として精度95%であった。  \nこのカーネルではただ基本的なものを試したわけだが、このデータ可視化と特徴量選択方法を使えば、95%以上の精度を簡単に出せるだろう。  \nそして他の分類手法にも使えるだろう。"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}