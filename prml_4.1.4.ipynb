{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRML 4.1.4~\n",
    "\n",
    "## 前回\n",
    "\n",
    "* 線形識別モデル\n",
    "* 識別関数\n",
    "* 2クラス,多クラス\n",
    "* 最小二乗法での課題\n",
    "  * 最小二乗法での分類では外れ値に敏感、領域を小さく判定してしまう。\n",
    "<img src=\"./images/Figure4.4b.jpg\" width=\"200px\" />\n",
    "<img src=\"./images/Figure4.5a.jpg\" width=\"200px\" />\n",
    "    \n",
    "             \n",
    "* 登場したパラメタと識別関数の関係（今回分でも使う）\n",
    "<img src=\"./images/Figure4.1.jpg\" width=\"300px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今回\n",
    "\n",
    "### フィッシャーの線形識別\n",
    "\n",
    "* 2クラスの場合で考える。\n",
    "* うまく分類できる$ y = \\vec{w}^T\\vec{x} $ のパラメタ$\\vec{w}$を求める。\n",
    "    * $\\vec{w}^T = (w_0,w_1,...,w_n)$ ベクトルを横に転置したもの\n",
    "    * $\\vec{x} $$ = \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      x_1 \\\\\n",
    "      x_2 \\\\\n",
    "      \\vdots \\\\\n",
    "      x_n \n",
    "    \\end{array}\n",
    "    \\right)\n",
    "    $\n",
    "    * うまく分類できる：判定後のクラス間の分離を大きくすることで考える\n",
    "\n",
    "### クラスの平均の差の最大化\n",
    "\n",
    "#### 平均ベクトル\n",
    "\n",
    "#### 制約付き最大化問題\n",
    "\n",
    "* ラグランジュの未定乗数法\n",
    "$L({x_n},\\lambda) = f(x_n) - \\lambda g(x_n) $ \n",
    "から$\\lambda$含めたパラメタそれぞれ微分して解く。\n",
    "\n",
    "* 最大化したい関数 : $f(\\vec{w}) = \\vec{w}^T(\\vec{m_2}-\\vec{m_1})$\n",
    "* 制約条件 : $g(\\vec{w}) = \\vec{w}^T\\vec{w} -1 = 0 $ \n",
    "※ $\\vec{w}$ は単位長 $\\Sigma_i w_i^2 =  1$\n",
    "\n",
    "* $ \\Sigma_i w_i^2\n",
    "= w_1^2 + w_2^2 + ... + w_n^2\n",
    "= (w_1, w_2,...,w_n)\\left(\n",
    "    \\begin{array}{c}\n",
    "    w_1 \\\\\n",
    "      w_2 \\\\\n",
    "      \\vdots \\\\\n",
    "      w_n \n",
    "    \\end{array}\n",
    "    \\right)\n",
    "= \\vec{w}^T\\vec{w}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 演習4.4\n",
    "\n",
    "$L = \\vec{w}^T(\\vec{m_2} - \\vec{m_1})  - \\lambda(\\vec{w}^T\\vec{w}-1) $\n",
    "\n",
    "   - $ \\frac{\\partial }{\\partial \\bf{w}} \\bf{w^Tw}$ <br />\n",
    "$ = (\\frac{\\partial }{\\partial \\bf{w}} \\bf{w^T})w + \\bf{w^T}(\\frac{\\partial }{\\partial \\bf{w}} w)$\n",
    "<br />\n",
    "$ = (1,1, ..., 1)\\bf{w} + \\bf{w^T}$$(1,1, ..., 1)^T$\n",
    "\n",
    "$ = \\bf{w + w} \\\\\n",
    "  = 2w\n",
    "$\n",
    "\n",
    "$\n",
    "\\frac{\\partial }{\\partial \\bf{w}} L \n",
    "= (\\bf{m_2 - m_1}$$)$  - $\\lambda(2\\bf{w} $$ - 0)\n",
    "$\n",
    "\n",
    "$\\frac{\\partial }{\\partial \\bf{w}} L = 0 $ とすると\n",
    "\n",
    "$(\\vec{m_2} - \\vec{m_1}) = \\lambda(2\\vec{w})$\n",
    "\n",
    "$\\vec{w} = \\frac{(\\vec{m_2} - \\vec{m1})}{2\\lambda} $\n",
    "\n",
    "$\n",
    "\\vec{w} \\propto{(\\vec{m_2} - \\vec{m1})} \n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クラス内の分散の最小化\n",
    "\n",
    "#### フィッシャーの判別基準\n",
    "\n",
    "* $S_B$ は対称行列 $S_B = S_B^{-T}$\n",
    "* $\\frac{\\partial }{\\partial \\bf{w}} (\\bf{w}^TAx)= (A+A^Tx)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Figure4.6a.jpg\" width=\"200px\" />\n",
    "<img src=\"./images/Figure4.6b.jpg\" width=\"200px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 参考\n",
    "\n",
    "- 画像(本家HP)\n",
    "    - http://research.microsoft.com/en-us/um/people/cmbishop/prml/webfigs.htm#ch4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
